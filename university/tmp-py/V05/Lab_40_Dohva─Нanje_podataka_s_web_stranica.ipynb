{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slike/vua.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U radu ponekad trebamo i podatke iz vanjskih izvora koje koristimo u programu.\n",
    "Izazov je u tome što nije uvijek moguće pronaći podatke koji su dostupni putem\n",
    "besplatnih API poziva. U ovakvim slučajevima moramo koristiti *web scraping*.\n",
    "*Web scraping* je način prikupljanja podataka koji se nalaze na web-stranicama.\n",
    "To se može učiniti ručno, kopiranjem sadržaja, ili programski (bot). Programski\n",
    "se podatci mogu prikupiti mnogo brže nego ručno i zato ćemo se usredotočiti na\n",
    "taj način. Trebamo uzeti u obzir da zakonitost ove prakse nije definirana.\n",
    "Web-stranice u svojim uvjetima korištenja i u datoteci *robots.txt* obično\n",
    "opisuju je li to dopušteno, ali imaju problem da kod prepoznavanja. *Web\n",
    "scraping* programi prikupljaju podatke s web-stranica na isti način kao što bi\n",
    "to učinio čovjek: otvore web-stranicu i dohvate podatke koje bi inače dohvatio\n",
    "web-preglednik. Svaka web-stranica ima drugačiju strukturu, zato za svaku moramo\n",
    "zasebno napraviti program. Web-stranice su kreirane pomoću jezika HTML (engl.\n",
    "*Hypertext Markup Language*), zajedno s CSS-om (engl. *Cascading Style Sheets*)\n",
    "i jezikom JavaScript. HTML elementi su odvojeni oznakama i izravno uvode sadržaj\n",
    "na web-stranicu. Evo kako izgleda osnovni HTML dokument:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slike/basic_html_page.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Možemo vidjeti da je sadržaj naslova sadržan između oznaka „h1“. Prvi paragraf\n",
    "nalazi se između oznaka „p“. Na pravoj web-stranici trebamo pronaći oznake s\n",
    "relevantnim podatcima i iskoristiti ih u svojem programu. Također, moramo\n",
    "odrediti koje veze (*linkove*) treba istražiti i gdje ih možemo pronaći među\n",
    "HTML datotekama. Uz sve ove informacije trebali bismo moći prikupiti potrebne\n",
    "podatke. Koristit ćemo modul *requests* iz prethodne vježbe i modul\n",
    "*BeautifulSoup*. Modul *requests* će omogućiti slanje HTTP zahtjeva i\n",
    "prihvaćanje odgovora (<http://docs.python-requests.org/en/master/>). Modul\n",
    "*BeautifulSoup* se koristiti za analizu HTML datoteka. To je jedan od najčešće\n",
    "korištenih modula za *web-scarping*. Jednostavan je za korištenje i ima mnogo\n",
    "značajki koje pomažu pri prikupljanju podataka iz HTML koda\n",
    "(<https://www.crummy.com/software/BeautifulSoup/bs4/doc/>).\n",
    "\n",
    "U ovom primjeru pokušat ćemo skupiti podatke s *online* knjižare:\n",
    "<http://books.toscrape.com/>. Ova web-stranica nije prava knjižara, već je\n",
    "napravljena za vježbanje skidanja podataka. Cilj nam je prikupiti podatke o\n",
    "proizvodima na web-stranici (naslov knjige, cijena, dostupnost, slika,\n",
    "kategorija i ocjena). Prvo upotrijebimo modul *requests* za dohvaćanje HTML koda\n",
    "glavne stranice i prikažimo dobiveni rezultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "glavni_url = \"http://books.toscrape.com/index.html\"\n",
    "odgovor = requests.get(glavni_url)\n",
    "odgovor.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rezultat je prilično neuredan! Učinimo ovo čitljivijim (nastavljamo s kodom dalje bez ponavljanja jer je dio već učitan):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(odgovor.text, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiramo funkciju za preuzimanje i analizu HTML koda jer će nam često trebati tijekom izvođenja programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def dohvatiStranicu(url):\n",
    "    odgovor = requests.get(url)\n",
    "    soup = BeautifulSoup(odgovor.text, 'html.parser')\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da bismo prikupili podatke o knjizi, moramo pristupiti stranici proizvoda. Prvi\n",
    "korak sastoji se od pronalaženja poveznica koje prikazuju svaku knjigu. U\n",
    "pregledniku otvorite glavnu stranicu web-mjesta, kliknite desnom tipkom miša na\n",
    "naziv proizvoda i kliknite na **inspect**. To će pokazati dio u HTML kodu\n",
    "web-stranice koji odgovara ovom elementu. Tako smo pronašli prvu vezu na knjigu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slike/inspect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovo možete pokušati sa svim drugim knjigama na stranici: struktura je uvijek\n",
    "ista. Veza proizvoda odgovara atributu „href\" oznake „a\". Ovo pripada oznaci\n",
    "„article“ s vrijednošću razreda „product_pod“. To će nam biti izvor za uočavanje\n",
    "URL-ova proizvoda. *BeautifulSoup* omogućuje da pronađemo one posebne oznake\n",
    "„article“. Funkciju *find()* možemo koristiti za pronalazak prve pojave ove\n",
    "oznake u HTML kodu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = dohvatiStranicu(\"http://books.toscrape.com/index.html\")\n",
    "soup.find(\"article\", class_ = \"product_pod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Još uvijek imamo previše informacija. Pokušajmo doći do URL-a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"article\", class_ = \"product_pod\").div.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puno bolje! Međutim, trebamo samo URL koji se nalazi u vrijednosti „href“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"article\", class_ = \"product_pod\").div.a.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uspjeli smo dobiti prvi URL proizvoda s modulom *BeautifulSoup*. Sada možemo iskoristiti funkciju *findAll()* i skupiti sve URL-ove proizvoda na glavnoj web-stranici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_page_products_urls = [x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "\n",
    "main_page_products_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ova funkcija je vrlo korisna za pronalaženje svih vrijednosti odjednom, ali\n",
    "morate provjeriti jesu li sve prikupljene informacije relevantne. Ponekad jedna\n",
    "te ista oznaka može sadržavati potpuno različite podatke. Zato je pri odabiru\n",
    "oznaka važno biti što precizniji. Ovdje smo se odlučili osloniti na oznaku\n",
    "„article“ s razredom „product_pod“ jer se čini da je to vrlo specifična oznaka i\n",
    "malo je vjerojatno da ćemo u njoj pronaći podatke koji nisu podatci o proizvodu.\n",
    "Prethodni URL-ovi odgovaraju njihovoj relativnoj stazi od glavne stranice. Da\n",
    "bismo ih dovršili, samo im moramo dodati URL glavne stranice:\n",
    "<http://books.toscrape.com/index.html> (nakon uklanjanja dijela index.html).\n",
    "Definirajmo novu funkciju za dohvaćanje veza knjiga na bilo kojoj stranici. To\n",
    "radimo u koracima, tako da provjerimo rezultat nakon svakog koraka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    print(soup)\n",
    "\n",
    "dohvatiURLove('http://books.toscrape.com/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    #pretvorimo url u listu s elementima odvojenim znakom '/'\n",
    "    url = (url.split(\"/\"))\n",
    "    print(url)\n",
    "   \n",
    "dohvatiURLove('http://books.toscrape.com/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    #pretvorimo url u listu s elementima odvojenim znakom '/'\n",
    "    url = (url.split(\"/\"))\n",
    "    #maknimo iz liste zadnji element (index.html)\n",
    "    url = url[:-1]\n",
    "    print(url)\n",
    "        \n",
    "dohvatiURLove('http://books.toscrape.com/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    #pretvorimo url u listu s elementima odvojenim znakom '/'\n",
    "    url = (url.split(\"/\"))\n",
    "    #maknimo iz liste zadnji element (index.html)\n",
    "    url = url[:-1]\n",
    "    #povežimo nazad u string\n",
    "    url = '/'.join(url)\n",
    "    print (url)\n",
    "    \n",
    "dohvatiURLove('http://books.toscrape.com/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    #pretvorimo url u listu s elementima odvojenim znakom '/'\n",
    "    url = (url.split(\"/\"))\n",
    "    #maknimo iz liste zadnji element (index.html)\n",
    "    url = url[:-1]\n",
    "    #povežimo nazad u string\n",
    "    url = '/'.join(url)\n",
    "    #kreiramo listu s cjelim URL-ovima\n",
    "    vrati = [url + '/' + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "    print (vrati)\n",
    "    \n",
    "dohvatiURLove('http://books.toscrape.com/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na kraju se nalazi funkcija kojoj prosljeđujemo URL stranice i ona pronalazi poveznice na sve proizvode i vraća ih kao listu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    #pretvorimo url u listu s elementima odvojenim znakom '/'\n",
    "    url = (url.split(\"/\"))\n",
    "    #maknimo iz liste zadnji element (index.html)\n",
    "    url = url[:-1]\n",
    "    #povežimo nazad u string\n",
    "    url = '/'.join(url)\n",
    "    #kreiramo listu s cjelim URL-ovima\n",
    "    vrati = [url + '/' + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "    return (vrati)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cijeli program trenutno izgleda ovako:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "glavni_url = \"http://books.toscrape.com/index.html\"\n",
    "\n",
    "def dohvatiStranicu(url):\n",
    "    odgovor = requests.get(url)\n",
    "    soup = BeautifulSoup(odgovor.text, 'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    #pretvorimo url u listu s elementima odvojenim znakom '/'\n",
    "    url = (url.split(\"/\"))\n",
    "    #maknimo iz liste zadnji element (index.html)\n",
    "    url = url[:-1]\n",
    "    #povežimo nazad u string\n",
    "    url = '/'.join(url)\n",
    "    #kreiramo listu s cjelim URL-ovima\n",
    "    vrati = [url + '/' + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "    return (vrati)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada pokušajmo dohvatiti URL-ove koji odgovaraju različitim kategorijama proizvoda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slike/inspect2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregledom vidimo da oni slijede isti uzorak URL-a: 'catalogue/category/books'.\n",
    "To možemo iskoristiti da dohvatimo URL-ove kategorija i pokrenuti modul *re*\n",
    "koji omogućuje da dodamo „catalogue/category/books\" kao dio URL-a koji tražimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "url_kategorija = [glavni_url + x.get('href') for x in soup.find_all(\"a\", href=re.compile(\"catalogue/category/books\"))]\n",
    "url_kategorija = url_kategorija[1:] # mičemo prvi zapis jer pokazuje na sve knjiga\n",
    "\n",
    "url_kategorija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uspjeli smo dohvatiti URL-ove 50 kategorija! Ne zaboravite uvijek provjeriti što\n",
    "ste dobili kao rezultat, kako biste bili sigurni da su sve informacije\n",
    "relevantne. Dobivanje URL-ova može biti korisno ako želimo preuzeti samo\n",
    "određenu kategoriju knjiga (proizvoda).\n",
    "\n",
    "Sad kad smo došli do poveznica, prikupimo podatke o knjigama. Znamo kako dobiti\n",
    "poveznicu na knjige unutar web-stranice. Sad ostaje još izazov kako doći do svih\n",
    "stranica na kojima su prikazane knjige. Obično se proizvodi prikazuju na više\n",
    "stranica ili na jednoj stranici, ali kroz pomicanje. Na dnu stranice vidimo da\n",
    "na konkretnom dućanu imamo 50 stranica i da se nalazimo na prvoj od njih."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slike/next.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na sljedećim stranicama se uz gumb *next* nalazi i *previous* za povratak na prethodnu stranicu s proizvodima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slike/previous_next.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kako bismo dohvatili sve URL-ove proizvoda, moramo biti u mogućnosti pregledati sve stranice. To možemo učiniti tako da iterativno pregledamo stranice koje se nalaze na gumbu *next*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slike/next_inspect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gumb *next* sadrži uzorak „*page*\". To možemo upotrijebiti za dohvaćanje URL-ova\n",
    "sljedećih stranica. Pritom budimo oprezni jer i gumb *previous* ima isti uzorak.\n",
    "Analizom koda možemo uočiti da u slučajevima u kojima imamo dva uzorka *page*,\n",
    "trebamo onaj koji se pojavljuje drugi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stranica = [glavni_url]\n",
    "\n",
    "dohvatiURL = dohvatiURLove(glavni_url)\n",
    "\n",
    "soup = dohvatiStranicu(glavni_url)\n",
    "\n",
    "# kada dobijemo dva podudaranja znači da smo na stranicama koje imaju prethodnu i sljedeću stranicu\n",
    "# kada imamo samo jedan rezultat nalazimo se na prvoj ili zadnjoj stranici\n",
    "# trebamo prestati kada dođemo do zadnje stranice\n",
    "\n",
    "while len(soup.findAll(\"a\", href=re.compile(\"page\"))) == 2 or len(url_stranica) == 1:\n",
    "    new_url = \"/\".join(url_stranica[-1].split(\"/\")[:-1]) + \"/\" + soup.findAll(\"a\", href=re.compile(\"page\"))[-1].get(\"href\")\n",
    "    url_stranica.append(new_url)\n",
    "    soup = dohvatiStranicu(new_url)\n",
    "    \n",
    "print(url_stranica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uspješno smo prikupili URL-ove 50 stranica. Zanimljivo je da je URL tih stranica\n",
    "vrlo predvidljiv. Mogli smo napraviti ovaj popis tako da povećavamo\n",
    "„page-X.html“ do 50. Ovo rješenje moglo bi funkcionirati za upravo ovaj primjer,\n",
    "ali više ne bi funkcioniralo ako se broj stranica promijeni. Jedno rješenje bi\n",
    "moglo biti povećanje vrijednosti dok ne dođemo na 404. stranicu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slike/404.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovdje možemo vidjeti da pokušaj odlaska na 51. stranicu ispisuje grešku 404.\n",
    "Srećom, *requests* zahtjeva ima atribut koji može pokazati status povratka HTML\n",
    "zahtjeva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odgovor = requests.get(\"http://books.toscrape.com/catalogue/page-50.html\")\n",
    "print(\"Status kod za stranicu 50: \" + str(odgovor.status_code))\n",
    "\n",
    "odgovor = requests.get(\"http://books.toscrape.com/catalogue/page-51.html\")\n",
    "print(\"Status kod za stranicu 51: \" + str(odgovor.status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U sljedećem koraku moramo dohvatiti URL-ove svih proizvoda na svim stranicama.\n",
    "Ovaj korak je vrlo jednostavan jer već imamo popis svih stranica s proizvodima i\n",
    "funkciju za dohvaćanje URL-ova proizvoda. Prelistajmo sve stranice i primijenimo\n",
    "svoju funkciju:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "glavni_url = \"http://books.toscrape.com/index.html\"\n",
    "\n",
    "def dohvatiStranicu(url):\n",
    "    odgovor = requests.get(url)\n",
    "    soup = BeautifulSoup(odgovor.text, 'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    #pretvorimo url u listu s elementima odvojenim znakom '/'\n",
    "    url = (url.split(\"/\"))\n",
    "    #maknimo iz liste zadnji element (index.html)\n",
    "    url = url[:-1]\n",
    "    #povežimo nazad u string\n",
    "    url = '/'.join(url)\n",
    "    #kreiramo listu s cjelim URL-ovima\n",
    "    vrati = [url + '/' + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "    return (vrati)\n",
    "\n",
    "def dohvatiSveStranice(url):\n",
    "    url_stranica = [url]\n",
    "    dohvatiURL = dohvatiURLove(glavni_url)\n",
    "    soup = dohvatiStranicu(glavni_url)\n",
    "    # kada dobijemo dva podudaranja znači da smo na stranicama koje imaju prethodnu i sljedeću stranicu\n",
    "    # kada imamo samo jedan rezultat nalazimo se na prvoj ili zadnjoj stranici\n",
    "    # trebamo prestati kada dođemo do zadnje stranice\n",
    "    while len(soup.findAll(\"a\", href=re.compile(\"page\"))) == 2 or len(url_stranica) == 1:\n",
    "        new_url = \"/\".join(url_stranica[-1].split(\"/\")[:-1]) + \"/\" + soup.findAll(\"a\", href=re.compile(\"page\"))[-1].get(\"href\")\n",
    "        url_stranica.append(new_url)\n",
    "        soup = dohvatiStranicu(new_url)\n",
    "    return (url_stranica)\n",
    "\n",
    "sve_stranice = dohvatiSveStranice(glavni_url)\n",
    "\n",
    "sve_knjige = []\n",
    "for stranica in sve_stranice:\n",
    "    sve_knjige.extend(dohvatiURLove(stranica))\n",
    "    \n",
    "print(len(sve_knjige))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napokon smo dobili 1000 URL-ovaza sve knjige koje se nalaze na web-mjestu.\n",
    "Stavimo to u funkciju i počnimo prikupljati podatke o pojedinim knjigama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "glavni_url = \"http://books.toscrape.com/index.html\"\n",
    "\n",
    "def dohvatiStranicu(url):\n",
    "    odgovor = requests.get(url)\n",
    "    soup = BeautifulSoup(odgovor.text, 'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    #pretvorimo url u listu s elementima odvojenim znakom '/'\n",
    "    url = (url.split(\"/\"))\n",
    "    #maknimo iz liste zadnji element (index.html)\n",
    "    url = url[:-1]\n",
    "    #povežimo nazad u string\n",
    "    url = '/'.join(url)\n",
    "    #kreiramo listu s cjelim URL-ovima\n",
    "    vrati = [url + '/' + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "    return (vrati)\n",
    "\n",
    "def dohvatiSveStranice(url):\n",
    "    url_stranica = [url]\n",
    "    dohvatiURL = dohvatiURLove(glavni_url)\n",
    "    soup = dohvatiStranicu(glavni_url)\n",
    "    # kada dobijemo dva podudaranja znači da smo na stranicama koje imaju prethodnu i sljedeću stranicu\n",
    "    # kada imamo samo jedan rezultat nalazimo se na prvoj ili zadnjoj stranici\n",
    "    # trebamo prestati kada dođemo do zadnje stranice\n",
    "    while len(soup.findAll(\"a\", href=re.compile(\"page\"))) == 2 or len(url_stranica) == 1:\n",
    "        new_url = \"/\".join(url_stranica[-1].split(\"/\")[:-1]) + \"/\" + soup.findAll(\"a\", href=re.compile(\"page\"))[-1].get(\"href\")\n",
    "        url_stranica.append(new_url)\n",
    "        soup = dohvatiStranicu(new_url)\n",
    "    return (url_stranica)\n",
    "\n",
    "def dohvatiSveLinkoveKnjiga (sve_stranice):\n",
    "    sve_knjige = []\n",
    "    for stranica in sve_stranice:\n",
    "        sve_knjige.extend(dohvatiURLove(stranica))\n",
    "    return (sve_knjige)\n",
    "\n",
    "sve_stranice = dohvatiSveStranice(glavni_url)\n",
    "\n",
    "sve_knjige = dohvatiSveLinkoveKnjiga(sve_stranice)\n",
    "\n",
    "   \n",
    "print(len(sve_knjige))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posljednji korak sastoji se od skupljanja podataka za svaku pojedinu knjigu.\n",
    "Istražimo najprije kako su informacije strukturirane na stranicama proizvoda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Slike/product_inspect.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skupimo sad sve podatke. Kako će ovo malo potrajati, ubacit ćemo i dio koji\n",
    "ispisuje vrijeme izvođenja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#kreiramo prazne liste za prikupljanje podataka\n",
    "naziv = []\n",
    "cijena = []\n",
    "zaliha = []\n",
    "url_slike = []\n",
    "kategorija = []\n",
    "ocjena = []\n",
    "# skidanje podataka o svim knjigama - traje malo duže jer treba posjetiti 1000 stranica\n",
    "for url in sve_knjige:\n",
    "    soup = dohvatiStranicu(url)\n",
    "    # naivi\n",
    "    naziv.append(soup.find(\"div\", class_ = re.compile(\"product_main\")).h1.text)\n",
    "    # cijena\n",
    "    cijena.append(soup.find(\"p\", class_ = \"price_color\").text[2:]) # makni oznaku valute\n",
    "    # zaliha\n",
    "    zaliha.append(re.sub(\"[^0-9]\", \"\", soup.find(\"p\", class_ = \"instock availability\").text)) # makni tekst\n",
    "    # link na sliku\n",
    "    url_slike.append(url.replace(\"index.html\", \"\") + soup.find(\"img\").get(\"src\"))\n",
    "    # kategorija\n",
    "    kategorija.append(soup.find(\"a\", href = re.compile(\"../category/books/\")).get(\"href\").split(\"/\")[3])\n",
    "    # ocjena\n",
    "    ocjena.append(soup.find(\"p\", class_ = re.compile(\"star-rating\")).get(\"class\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na kraju to spremimo u novu funkciju te napravimo izlaz iz funkcije koja će\n",
    "vratiti tablicu sa svim podatcima korištenjem modula *pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, pandas\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def dohvatiStranicu(url):\n",
    "    odgovor = requests.get(url)\n",
    "    soup = BeautifulSoup(odgovor.text, 'html.parser')\n",
    "    return(soup)\n",
    "\n",
    "def dohvatiURLove(url):\n",
    "    soup = dohvatiStranicu(url)\n",
    "    #pretvorimo url u listu s elementima odvojenim znakom '/'\n",
    "    url = (url.split(\"/\"))\n",
    "    #maknimo iz liste zadnji element (index.html)\n",
    "    url = url[:-1]\n",
    "    #povežimo nazad u string\n",
    "    url = '/'.join(url)\n",
    "    #kreiramo listu s cjelim URL-ovima\n",
    "    vrati = [url + '/' + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "    return (vrati)\n",
    "\n",
    "def dohvatiSveStranice(url):\n",
    "    url_stranica = [url]\n",
    "    dohvatiURL = dohvatiURLove(glavni_url)\n",
    "    soup = dohvatiStranicu(glavni_url)\n",
    "    # kada dobijemo dva podudaranja znači da smo na stranicama koje imaju prethodnu i sljedeću stranicu\n",
    "    # kada imamo samo jedan rezultat nalazimo se na prvoj ili zadnjoj stranici\n",
    "    # trebamo prestati kada dođemo do zadnje stranice\n",
    "    while len(soup.findAll(\"a\", href=re.compile(\"page\"))) == 2 or len(url_stranica) == 1:\n",
    "        new_url = \"/\".join(url_stranica[-1].split(\"/\")[:-1]) + \"/\" + soup.findAll(\"a\", href=re.compile(\"page\"))[-1].get(\"href\")\n",
    "        url_stranica.append(new_url)\n",
    "        soup = dohvatiStranicu(new_url)\n",
    "    return (url_stranica)\n",
    "\n",
    "def dohvatiSveLinkoveKnjiga (glavni_url):\n",
    "    sve_stranice = dohvatiSveStranice(glavni_url)\n",
    "    sve_knjige = []\n",
    "    for stranica in sve_stranice:\n",
    "        sve_knjige.extend(dohvatiURLove(stranica))\n",
    "    return (sve_knjige)\n",
    "\n",
    "def dohvatiSvePodatke(glavni_url):\n",
    "    sve_knjige = dohvatiSveLinkoveKnjiga (glavni_url)\n",
    "    naziv = []\n",
    "    cijena = []\n",
    "    zaliha = []\n",
    "    url_slike = []\n",
    "    kategorija = []\n",
    "    ocjena = []\n",
    "    # skidanje podataka o svim knjigama - traje malo duže jer treba posjetiti 1000 stranica\n",
    "    for url in sve_knjige:\n",
    "        soup = dohvatiStranicu(url)\n",
    "        # naivi\n",
    "        naziv.append(soup.find(\"div\", class_ = re.compile(\"product_main\")).h1.text)\n",
    "        # cijena\n",
    "        cijena.append(soup.find(\"p\", class_ = \"price_color\").text[2:]) # get rid of the pound sign\n",
    "        # zaliha\n",
    "        zaliha.append(re.sub(\"[^0-9]\", \"\", soup.find(\"p\", class_ = \"instock availability\").text)) # get rid of non numerical characters\n",
    "        # link na sliku\n",
    "        url_slike.append(url.replace(\"index.html\", \"\") + soup.find(\"img\").get(\"src\"))\n",
    "        # kategorija\n",
    "        kategorija.append(soup.find(\"a\", href = re.compile(\"../category/books/\")).get(\"href\").split(\"/\")[3])\n",
    "        # ocjena\n",
    "        ocjena.append(soup.find(\"p\", class_ = re.compile(\"star-rating\")).get(\"class\")[1])\n",
    "    \n",
    "    preuzeti_podaci = pandas.DataFrame({'naziv': naziv, \n",
    "                                    'cijena': cijena, \n",
    "                                    'zaliha': zaliha, \n",
    "                                    \"url_slike\": url_slike, \n",
    "                                    \"kategorija\": kategorija, \n",
    "                                    \"ocjena\": ocjena})\n",
    "    return(preuzeti_podaci)\n",
    "    \n",
    "\n",
    "glavni_url = \"http://books.toscrape.com/index.html\"\n",
    "podaci = dohvatiSvePodatke(glavni_url)\n",
    "podaci\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U tablici sad imamo 1000 redaka i 6 stupaca.  \n",
    "**Ovim smo završili naš seminar iz uvoda u Python.**   \n",
    "Ostaje vam još zadnji zadatak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=\"alert alert-info\"><b>Vježba</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napišite program koji će u obliku tablice vratiti podatke o temperaturi mora.  \n",
    "Web-stranica se nalazi na poveznici https://meteo.hr/podaci.php?section=podaci_vrijeme&param=more_n.  \n",
    "U radu na zadatku u prvoj ćeliji riješite dio koji se tiče učitavanja HTML koda sa stranice, pa nastavite u idućoj ćeliji raditi na programu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# učitavanje HTML-a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obrada preuzetog koda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vizualizacija podataka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=\"alert alert-info\"><b>Kraj</b></div></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
